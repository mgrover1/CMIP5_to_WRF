{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pseudo Global Warming Climate Delta Calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in necccessary libraries and packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "from matplotlib import pyplot as plt\n",
    "import xarray as xr\n",
    "import Ngl\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "from dask_jobqueue import SLURMCluster\n",
    "from dask.distributed import Client\n",
    "\n",
    "import xesmf as xe\n",
    "\n",
    "import metpy\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start up dask to run functions in parallel to speed things up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = SLURMCluster()\n",
    "cluster.scale(2)\n",
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in the NAM data which will be used for regridding and vertical interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nam_including_coords_ds = xr.open_dataset('/data/meso/a/jtrapp/formax/namanl_218_20100510_1200_000.grb', engine='cfgrib',\n",
    "                                           backend_kwargs={\n",
    "                                                            'filter_by_keys': {'cfName': 'eastward_wind', 'typeOfLevel': 'isobaricInhPa'},\n",
    "                                                            'errors': 'ignore'\n",
    "                                                            }\n",
    "                                         )\n",
    "ds_out = nam_including_coords_ds.rename({'isobaricInhPa': 'plev', 'longitude': 'lon', 'latitude': 'lat'})\n",
    "\n",
    "plev_out = ds_out.plev.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Setup a few functions to make things easier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_vert_coords(ds, var, vert_levs, ds_ps=None):\n",
    "    \n",
    "    # This section will work with GFDL and MIROC since they have the same naming conventions\n",
    "    try:\n",
    "        \n",
    "        # Flip the data to span from top to bottom instead of bottom to top (of atmosphere)\n",
    "        data = np.flip(ds[var].values, axis=1) \n",
    "        \n",
    "        time = ds.time.values\n",
    "        \n",
    "        # Read in the a and b coefficients used for converting from hybrid-sigma coordiantes to pressure levels\n",
    "        a = ds['a'].values[::-1]\n",
    "        b = ds['b'].values[::-1]\n",
    "        \n",
    "        # Use the pressure surface provided in the file\n",
    "        ps = ds['ps'].values\n",
    "        \n",
    "        # Read in lats and lons\n",
    "        lat = ds['lat'].values\n",
    "        lon = ds['lon'].values\n",
    "        \n",
    "        # Convert from hybrid sigma coordinates to pressure coordinates using PyNGL\n",
    "        val = Ngl.vinth2p(datai=data, hbcofa=a, hbcofb=b, plevo=vert_levs, psfc=ps,intyp=2, p0=1013.25, ii=1, kxtrp=False)\n",
    "        \n",
    "        # Output to an array\n",
    "        new_ds = xr.DataArray(val, coords=[time, vert_levs, lat, lon], dims=['time', 'plev', 'lat', 'lon']).to_dataset(name=var)\n",
    "        \n",
    "    except:\n",
    "        \n",
    "        # Follows the same process as above, except uses separate pressure array\n",
    "        data = ds[var].values\n",
    "        time = ds.time.values\n",
    "        a = ds['hyam'].values\n",
    "        b = ds['hybm'].values\n",
    "        ps = ds_ps['PS'].values\n",
    "        lat = ds['lat'].values\n",
    "        lon = ds['lon'].values\n",
    "    \n",
    "        val = Ngl.vinth2p(datai=data, hbcofa=a, hbcofb=b, plevo=vert_levs, psfc=ps, intyp=2, p0=1000, ii=1, kxtrp=False)\n",
    "    \n",
    "        new_ds = xr.DataArray(val, coords=[time, vert_levs, lat, lon], dims=['time', 'plev', 'lat', 'lon']).to_dataset(name=var)\n",
    "    \n",
    "    # Remove extremely large values and replace with nan\n",
    "    return new_ds.where(new_ds[var] < 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regrid(ds_in, ds_out, plev_out=None, vert_interp=True):\n",
    "    \n",
    "    # Regrids using NAM data read in from above\n",
    "    # Set vert_interp to true when working with pressure level data, otherwise set to False for surface data\n",
    "    # Use log-P to interpolate\n",
    "    \n",
    "    if vert_interp==True:\n",
    "        # Reduce the vertical extent to 50-hPa instead of 10-hPa\n",
    "        reduced_ds = ds_in.isel(plev=np.where(ds_in.plev.values >= 50)[0])\n",
    "    \n",
    "        # Change the vertical coordinate to ln(p)\n",
    "        reduced_ds['plev'] = np.log(reduced_ds['plev'].values)\n",
    "    \n",
    "        # Change the vertical coordinate of the new grid to ln(p)\n",
    "        plev_out = np.log(ds_out.plev.values)\n",
    "    \n",
    "        # Perform the interpolation\n",
    "        climo_delta_interpolated = reduced_ds.interp({'plev': plev_out})\n",
    "        \n",
    "        # Change the pressure values back to the original values\n",
    "        climo_delta_interpolated['plev'] = ds_out.plev.values\n",
    "    \n",
    "        # Change the name of the pressure levels to isobaric to match NAM\n",
    "        climo_delta = climo_delta_interpolated.rename({'plev':'isobaric'})\n",
    "        \n",
    "    else:\n",
    "        climo_delta = ds_in\n",
    "    \n",
    "    regridder = xe.Regridder(climo_delta, ds_out, 'bilinear', reuse_weights=True)\n",
    "    \n",
    "    return regridder(climo_delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interp_hourly(ds):\n",
    "    \n",
    "    # Interpolates the three hourly data, removing the 24 hour value since we only want up to 21\n",
    "    ds_copy = ds.sel(hour=0)\n",
    "    ds_copy['hour'] = 24\n",
    "    return xr.concat([ds, ds_copy], dim='hour').interp({'hour':np.arange(0, 24, 3)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_decade_data(model, var, list_years, month):\n",
    "    \n",
    "    # Create a list for the different datasets\n",
    "    ds_list = []\n",
    "    \n",
    "    # With NCAR data, pressure surface data needs to be read in seperately\n",
    "    if model == 'NCAR':\n",
    "        ds_ps_list = []\n",
    "    \n",
    "        # Create a dictionary for the variable names for NCAR\n",
    "        ncar_var_names = {'ta':'T',\n",
    "                          'ua':'U',\n",
    "                          'va':'V',\n",
    "                          'hus':'Q'}\n",
    "        \n",
    "        # Rename the NCAR variables to match those in the files\n",
    "        var = ncar_var_names[var]\n",
    "    \n",
    "    # Read in the files using typical naming conventions\n",
    "    files = sorted(glob.glob('/data/meso/a/mgrover4/'+model+'_6hour/'+var+'_*'))\n",
    "    \n",
    "    \n",
    "    # If the file list is empty, try using another file naming convention (typically happens with NCAR data)\n",
    "    if len(files) == 0:\n",
    "        \n",
    "        files = sorted(glob.glob('/data/meso/a/mgrover4/'+model+'_6hour/*.'+var+'.nc'))\n",
    "    \n",
    "    if model == 'MIROC':\n",
    "        plevs = xr.open_dataset('/data/jtrapp/a/jtrapp/CMIP5/MIROC/ta_Amon_MIROC5_historical_r1i1p1_199001-199912.nc').plev.values/100\n",
    "        \n",
    "    elif model == 'GFDL':\n",
    "        plevs = xr.open_dataset('/data/jtrapp/a/jtrapp/CMIP5/GFDL/ta_Amon_GFDL-CM3_historical_r1i1p1_199001-199412.nc').plev.values/100\n",
    "        \n",
    "    elif model == 'NCAR':\n",
    "        plevs = xr.open_dataset('/data/jtrapp/a/jtrapp/CMIP5/NCAR/ta_Amon_CCSM4_historical_r1i1p1_195001-200512.nc').plev.values/100\n",
    "        ps_files = sorted(glob.glob('/data/meso/a/mgrover4/'+model+'_6hour/*.PS.nc'))\n",
    "        \n",
    "    for year in list_years:\n",
    "        \n",
    "        # Convert the year into a string\n",
    "        year = str(year)\n",
    "        \n",
    "        # Loop through the different files\n",
    "        for i in range(len(files)):\n",
    "            \n",
    "            # For the MIROC data, 3D atmos variables are stored in monthly files, not yearly so different datetime is used\n",
    "            if (model == 'MIROC') and ((var == 'ta') or (var == 'hus') or (var == 'ua') or (var == 'va')):\n",
    "                datetime = year+month\n",
    "            \n",
    "            # For any data other than MIROC, use just the year\n",
    "            else:\n",
    "                datetime = year\n",
    "            \n",
    "            if datetime in files[i]:\n",
    "                print(files[i])\n",
    "                \n",
    "                # Open dataset\n",
    "                ds = xr.open_dataset(files[i]).sel(time = year+ '-' +month)\n",
    "                \n",
    "                # If NCAR dataset, read in the PS values\n",
    "                if model == 'NCAR':\n",
    "                    ds_ps = xr.open_dataset(ps_files[i]).sel(time = year+ '-' +month)\n",
    "                    ds = convert_vert_coords(ds, var, plevs, ds_ps=ds_ps)\n",
    "                \n",
    "                else:\n",
    "                    \n",
    "                    # Convert from hybrid-sigma coordinates to pressure coordinates\n",
    "                    ds = convert_vert_coords(ds, var, plevs)\n",
    "            \n",
    "                # Find hourly average from the full month of data\n",
    "                ds = ds.groupby('time.hour').mean('time')\n",
    "            \n",
    "                # Linearly interpolate hourly data\n",
    "                ds = interp_hourly(ds)\n",
    "            \n",
    "                # Add to the historical list of datasets\n",
    "                ds_list.append(ds)\n",
    "            \n",
    "                print('Done with: '+year + ' ' + var)\n",
    "    \n",
    "    # Merge the years together\n",
    "    ds = xr.concat(ds_list, dim='hour')\n",
    "    \n",
    "    # Finish with the horizontal regridding and log-p vertical interpolation\n",
    "    ds = regrid(ds, ds_out, plev_out)\n",
    "    \n",
    "    # Average across the different hours\n",
    "    return ds.groupby('hour').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def atmos_delta(var, model='MIROC', month='05', historical_years=list(range(1990, 2000)), future_years=list(range(2090, 2100))):\n",
    "    \n",
    "    # Calculate the delta using this function - make sure to use different month values for March (default is May)\n",
    "    \n",
    "    # Calulate historical average\n",
    "    historical_ds = process_decade_data(model, var, historical_years, month)\n",
    "    \n",
    "    # Calculate future average\n",
    "    future_ds = process_decade_data(model, var, future_years, month)\n",
    "    \n",
    "    return future_ds - historical_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "## Calculate 3D Atmos Deltas\n",
    "\n",
    "The default month in this section is the month of May, so if you would like to calculate another month, please input the month in the ```atmos_delta()``` funtion (ex. ```month='03'```) for March"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MIROC 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of variables of 3D atmos deltas\n",
    "atmos_vars = ['ta', 'ua', 'va', 'hus']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list for the datasets\n",
    "miroc_ds_list = []\n",
    "\n",
    "# Loop through the different variables and calculate the PGW climate delta\n",
    "for var in atmos_vars:\n",
    "    miroc_ds_list.append(atmos_delta(var, 'MIROC').compute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge into a single dataset\n",
    "miroc_3d_atmos = xr.merge(miroc_ds_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to file\n",
    "miroc_3d_atmos.to_netcdf('../preliminary_netcdf/MIROC_3D_vars_3hr.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GFDL-CM3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list for the datasets\n",
    "gfdl_ds_list = []\n",
    "\n",
    "# Loop through the different variables and calculate the PGW climate delta\n",
    "for var in atmos_vars:\n",
    "    gfdl_ds_list.append(atmos_delta(var, 'GFDL'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the datasets\n",
    "gfdl_3d_atmos = xr.merge(gfdl_ds_list)\n",
    "\n",
    "# Write to file\n",
    "gfdl_3d_atmos.to_netcdf('../preliminary_netcdf/GFDL_3D_vars_3hr.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NCAR-CCSM4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create  list for the datasets\n",
    "ncar_ds_list = []\n",
    "\n",
    "# Loop through the different variables and calculate the PGW climate delta\n",
    "for var in atmos_vars:\n",
    "    ncar_ds_list.append(atmos_delta(var, 'NCAR').compute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the different variable datasets into one\n",
    "ncar_3d_atmos = xr.merge(ncar_ds_list)\n",
    "\n",
    "# Change names to match naming convention from MIROC and GFDL\n",
    "ncar_3d_atmos.rename({'T':'ta',\n",
    "                      'U':'ua',\n",
    "                      'V':'va',\n",
    "                      'Q':'hus'})\n",
    "\n",
    "# Write to netcdf file in preliminary netcdf - used in the write to grib\n",
    "ncar_3d_atmos.to_netcdf('../preliminary_netcdf/NCAR_3D_vars_3hr.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Surface Deltas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_data(var, model, month=5, var_type='surface', plev_out=plev_out):\n",
    "    \n",
    "    # Pull in the files\n",
    "    if 'NCAR' in model:\n",
    "        \n",
    "        files = sorted(glob.glob('/data/meso/a/mgrover4/'+model+'_6hour/*.'+var+'.nc'))\n",
    "        \n",
    "        if (var == 'T') or (var == 'Q'):\n",
    "            \n",
    "            ds_list = []\n",
    "                       \n",
    "            # Loop through the different time steps and use the lowest sigma level for temperature \n",
    "            for file in files:\n",
    "                ds = xr.open_dataset(file, chunks={'time': 40}).isel(lev=-1)[var].to_dataset()\n",
    "                #ds = ds.sel(time = ds['time.month'] == 5)\n",
    "                ds_list.append(ds)\n",
    "            \n",
    "            # Merge all the time steps\n",
    "            ds = xr.concat(ds_list, dim='time')\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            ds_list = []\n",
    "            \n",
    "            for file in files:\n",
    "                ds = xr.open_dataset(file, chunks={'time': 40})[var].to_dataset()\n",
    "                ds_list.append(ds)\n",
    "            ds = xr.concat(ds_list, dim='time')\n",
    "    else:\n",
    "        \n",
    "        files = sorted(glob.glob('/data/meso/a/mgrover4/'+model+'_6hour/'+var+'_*'))\n",
    "        ds = xr.open_mfdataset(files, chunks={'time': 40}, concat_dim='time', combine='nested')\n",
    "    \n",
    "    # Select may\n",
    "    may_ds = ds.sel(time = (ds['time.month'] == month))\n",
    "        \n",
    "    # Subset historical years\n",
    "    historical_ds = may_ds.sel(time = (may_ds['time.year'] < 2000))\n",
    "    \n",
    "    # Average historical - hourly\n",
    "    historical_ds_mean_hourly = historical_ds.groupby('time.hour').mean('time').compute()\n",
    "    print('processed hourly historical')\n",
    "    \n",
    "    # Subset future years\n",
    "    future_ds = may_ds.sel(time = (may_ds['time.year'] > 2000))\n",
    "    \n",
    "    # Average future - hourly\n",
    "    future_ds_mean_hourly = future_ds.groupby('time.hour').mean('time').compute()\n",
    "    print('processed hourly future')\n",
    "    \n",
    "    \n",
    "    # Find difference between future and past averages - hourly\n",
    "    delta_ds_hourly = future_ds_mean_hourly[var] - historical_ds_mean_hourly[var]\n",
    "    \n",
    "    return delta_ds_hourly.to_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_surface(model, variables, plot=False):\n",
    "    \n",
    "    datasets_hourly = []\n",
    "    datasets_monthly = []\n",
    "    for var in variables:\n",
    "        ds_hourly = average_data(var, model)\n",
    "        \n",
    "        # Interpolate to 3-hourly if it is 6 hour data\n",
    "        if 21 not in ds_hourly.hour.values:\n",
    "            ds_hourly = interp_hourly(ds_hourly)\n",
    "\n",
    "        datasets_hourly.append(ds_hourly)\n",
    "\n",
    "        print('Processed '+var)\n",
    "    \n",
    "    # Merge hourly datasets\n",
    "    delta_ds_hourly = xr.merge(datasets_hourly, compat='override')\n",
    "    print(delta_ds_hourly)\n",
    "    \n",
    "    # Create Plots\n",
    "    \n",
    "    if plot:\n",
    "        for hour in delta_ds_hourly.hour.values:\n",
    "            if 'NCAR' in model:\n",
    "                surface_plot(delta_ds_hourly, hour, model+' Surface')\n",
    "            else:\n",
    "                surface_plot(delta_ds_hourly, hour, model+' Surface')\n",
    "    \n",
    "    return delta_ds_hourly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interp_hourly(ds):\n",
    "    ds_copy = ds.sel(hour=0)\n",
    "    ds_copy['hour'] = 24\n",
    "    return xr.concat([ds, ds_copy], dim='hour').interp({'hour':np.arange(0, 24, 3)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Calculate Surface Deltas\n",
    "\n",
    "### MIROC 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "miroc_hourly_ds = process_surface('MIROC', ['ps', 'psl', 'tas', 'uas', 'vas', 'huss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GFDL-CM3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gfdl_hourly_ds = process_surface('GFDL', ['ps', 'psl', 'tas', 'uas', 'vas', 'huss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NCAR-CCSM4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncar_hourly_ds = process_surface('NCAR', ['T', 'Q', 'PS', 'PSL', 'TS', 'UBOT', 'VBOT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename NCAR variables to match naming convention from MIROC and GFDL\n",
    "ncar_hourly_ds = ncar_hourly_ds.rename({'T':'tas',\n",
    "                                        'Q':'huss',\n",
    "                                        'PS':'ps',\n",
    "                                        'PSL':'psl',\n",
    "                                        'TS':'ts',\n",
    "                                        'UBOT':'uas',\n",
    "                                        'VBOT':'vas'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write all the surface deltas to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regrid(miroc_hourly_ds, ds_out, vert_interp=False).to_netcdf('../preliminary_netcdf/MIROC_2d_vars_3hr.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regrid(gfdl_hourly_ds, ds_out, vert_interp=False).to_netcdf('../preliminary_netcdf/GFDL_2d_vars_3hr.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regrid(ncar_hourly_ds, ds_out, vert_interp=False).to_netcdf('../preliminary_netcdf/NCAR_2d_vars_3hr.nc')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
